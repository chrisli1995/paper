default_method_hyperparameters:
  num_steps: 500
  adv_string_init : "! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !"
  allow_non_ascii: False
  search_width: 512
  eval_steps: 10
  use_prefix_cache: True
  targets_path: ./data/optimizer_targets/harmbench_targets_text.json

# ========== Specific experiment hyperparameters (overrides default_method_hyperparameters) ========== #
# Experiment names can be used in the save path and to override default hyperparameters.
# To add a new experiment, add a new section with the following format:
# 
# experiment_name:
#   <hyperparameter>: <value>
#   ...
#   <hyperparameter>: <value>
# 
# Experiment name parsing:
# If an experiment name contains <model_name#>, then it will be expanded out into all model names in the models.yaml file.
# The parameters in these experiment configs can have the form <model_name#>['<key1>'][index1]['<key2>']... to reference values in the corresponding model config.
# If an expanded experiment name would have the same name as a manually added experiment, the manually added experiment config will be used instead.

<model_name1>:
  target_models:
    - target_model: <model_name1>['model']
      num_gpus: <model_name1>['num_gpus']

1000steps_<model_name1>:
  num_steps: 1000
  target_models:
    - target_model: <model_name1>['model']
      num_gpus: <model_name1>['num_gpus']

llama2_70b:
  target_models:
    - target_model: <model_name1>['model']
      num_gpus: 4

qwen_7b_chat:
  target_models:
    - target_model: <model_name1>['model']
      num_gpus: 1
  use_prefix_cache: False  # Unknown error causes issues when using KV cache during optimization with Qwen

qwen_14b_chat:
  target_models:
    - target_model: <model_name1>['model']
      num_gpus: 1
  use_prefix_cache: False  # Unknown error causes issues when using KV cache during optimization with Qwen

qwen_72b_chat:
  target_models:
    - target_model: <model_name1>['model']
      num_gpus: 4
  use_prefix_cache: False  # Unknown error causes issues when using KV cache during optimization with Qwen

baichuan2_7b:
  target_models:
    - target_model: <model_name1>['model']
      num_gpus: 1
  use_prefix_cache: False  # Baichuan 2 expects KV cache to only be used during generation

baichuan2_13b:
  target_models:
    - target_model: <model_name1>['model']
      num_gpus: 1
  use_prefix_cache: False  # Baichuan 2 expects KV cache to only be used during generation

# Note: Multi-model optimization requires identical tokenizers and dictionaries
llama2_7b_vicuna_7b_llama2_13b_vicuna_13b_multibehavior_1000steps:
  num_steps: 1000
  target_models:
    - target_model:
        model_name_or_path: meta-llama/Llama-2-7b-chat-hf
        dtype: fp16
        chat_template: llama-2
      num_gpus: 1
    - target_model: 
        model_name_or_path: lmsys/vicuna-7b-v1.3  # use 1.3 to follow the GCG paper's transfer experiments
        dtype: fp16
        chat_template: vicuna
      num_gpus: 1
    - target_model:
        model_name_or_path: meta-llama/Llama-2-13b-chat-hf
        dtype: fp16
        chat_template: llama-2
      num_gpus: 1
    - target_model: 
        model_name_or_path: lmsys/vicuna-13b-v1.3
        dtype: fp16
        chat_template: vicuna
      num_gpus: 1
