default_method_hyperparameters:
  num_test_cases_per_behavior: 5
  test_cases_batch_size: 5
  num_steps: 500
  lr: 0.01
  num_optim_tokens: 20
  targets_path: ./data/optimizer_targets/harmbench_targets_text.json

# ========== Specific experiment hyperparameters (overrides default_method_hyperparameters) ========== #
# Experiment names can be used in the save path and to override default hyperparameters.
# To add a new experiment, add a new section with the following format:
# 
# experiment_name:
#   <hyperparameter>: <value>
#   ...
#   <hyperparameter>: <value>
# 
# Experiment name parsing:
# If an experiment name contains <model_name#>, then it will be expanded out into all model names in the models.yaml file.
# The parameters in these experiment configs can have the form <model_name#>['<key1>'][index1]['<key2>']... to reference values in the corresponding model config.
# If an expanded experiment name would have the same name as a manually added experiment, the manually added experiment config will be used instead.

<model_name1>:
  target_model: <model_name1>['model']

llama2_70b:
  target_model: <model_name1>['model']
  test_cases_batch_size: 1  # to avoid OOM error
  num_test_cases_per_behavior: 1  # 1 is sufficient, and 70b is expensive

qwen_72b_chat:
  target_model: <model_name1>['model']
  test_cases_batch_size: 1  # to avoid OOM error
  num_test_cases_per_behavior: 1  # 1 is sufficient, and 70b is expensive